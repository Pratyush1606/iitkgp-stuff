{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment9_18EE35014.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcoFCVeqZnCd"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from __future__ import division\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFzONbPzZu6o",
        "outputId": "c0c67010-5381-4454-92c7-ed2573fa5dab"
      },
      "source": [
        "!pip install numba"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (0.51.2)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from numba) (1.19.5)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba) (57.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVm7I_oXZy6t"
      },
      "source": [
        "from numba import cuda, float32\n",
        "import math\n",
        "import time"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pv9tZmzafLw"
      },
      "source": [
        "A = np.random.rand(100,1000)\n",
        "B = np.random.rand(1000,100)\n",
        "C= np.zeros([100,100])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ej_kPQukZ2os"
      },
      "source": [
        "def norm_CPU_multiplication(A,B,C):\n",
        "  for i in range(len(A)):\n",
        "    for j in range(len(B[0])):\n",
        "      for k in range(len(B)):\n",
        "        C[i][j] += A[i][k]*B[k][j]\n",
        "  "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22TJ0nQCaZ1P",
        "outputId": "8acaa4ee-278a-40fd-d807-d4ec711c7b03"
      },
      "source": [
        "t1=time.time()\n",
        "norm_CPU_multiplication(A,B,C)\n",
        "t2=time.time()\n",
        "print(f\"Time taken using CPU: {t2-t1} s\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken using CPU: 14.474081993103027 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPe0eQAyamEi",
        "outputId": "748e838a-33ab-4122-fc13-15dbc3b1a4c1"
      },
      "source": [
        "cuda.detect()\n",
        "TPB = 16"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 CUDA devices\n",
            "id 0            b'Tesla K80'                              [SUPPORTED]\n",
            "                      compute capability: 3.7\n",
            "                           pci device id: 4\n",
            "                              pci bus id: 0\n",
            "Summary:\n",
            "\t1/1 devices are supported\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50mi_8TaeT1x"
      },
      "source": [
        "@cuda.jit\n",
        "def fast_matmul(A, B, C):\n",
        "    \"\"\"\n",
        "    Perform matrix multiplication of C = A * B\n",
        "    Each thread computes one element of the result matrix C\n",
        "    \"\"\"\n",
        "    # Define an array in the shared memory\n",
        "    # The size and type of the arrays must be known at compile time\n",
        "    sA = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
        "    sB = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
        "    x, y = cuda.grid(2)\n",
        "    tx = cuda.threadIdx.x\n",
        "    ty = cuda.threadIdx.y\n",
        "    if x >= C.shape[0] and y >= C.shape[1]:\n",
        "        return\n",
        "    # Each thread computes one element in the result matrix.\n",
        "    # The dot product is chunked into dot products of TPB-long vectors.\n",
        "    tmp = 0.\n",
        "    for i in range(int(A.shape[1] / TPB)):\n",
        "        # Preload data into shared memory\n",
        "        sA[tx, ty] = A[x, ty + i * TPB]\n",
        "        sB[tx, ty] = B[tx + i * TPB, y]\n",
        "\n",
        "        # Wait until all threads finish preloading\n",
        "        cuda.syncthreads()\n",
        "\n",
        "        # Computes partial product on the shared memory\n",
        "        for j in range(TPB):\n",
        "            tmp += sA[tx, j] * sB[j, ty]\n",
        "        # Wait until all threads finish computing\n",
        "        cuda.syncthreads()\n",
        "    C[x, y] = tmp"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RO1CUNdBd2kB",
        "outputId": "b4e3b68e-ba1b-461f-829c-3b6acacf1ff7"
      },
      "source": [
        "# Copy the arrays to the device\n",
        "A_global_mem = cuda.to_device(A)\n",
        "B_global_mem = cuda.to_device(B)\n",
        "\n",
        "# Allocate memory on the device for the result\n",
        "C_global_mem = cuda.to_device(C)\n",
        "\n",
        "# Configure the blocks\n",
        "threadsperblock = (16, 16)\n",
        "blockspergrid_x = int(math.ceil(A.shape[0] / threadsperblock[0]))\n",
        "blockspergrid_y = int(math.ceil(B.shape[1] / threadsperblock[1]))\n",
        "blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
        "\n",
        "# Start the kernel\n",
        "t1 = time.time() \n",
        "%time fast_matmul[blockspergrid, threadsperblock](A_global_mem, B_global_mem, C_global_mem)\n",
        "t2 = time.time()\n",
        "print(f\"Time taken using Cuda enabled GPU: {t2-t1} s\")\n",
        "cuda.synchronize()\n",
        "# Copy the result back to the host\n",
        "C = C_global_mem.copy_to_host()\n",
        "print(C)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 486 ms, sys: 139 ms, total: 625 ms\n",
            "Wall time: 839 ms\n",
            "Time taken using Cuda enabled GPU: 0.840301513671875 s\n",
            "[[250.87959658 243.27234536 240.74487816 ... 242.0475735  240.10817654\n",
            "  246.740573  ]\n",
            " [251.52568975 242.37081779 247.75175142 ... 240.31095855 240.07474709\n",
            "  239.41254866]\n",
            " [251.75000071 240.23767165 239.79359701 ... 233.8446009  233.4805479\n",
            "  241.76428818]\n",
            " ...\n",
            " [251.99175769 241.71379013 243.3729617  ... 239.13409464 242.71306458\n",
            "  243.17882511]\n",
            " [271.63980474 249.18459253 251.42040772 ... 253.80387899 255.23206535\n",
            "  257.97255593]\n",
            " [261.84900164 245.73811686 244.38043513 ... 247.21284295 250.31213379\n",
            "  252.36456222]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_bvsPmpkPTd"
      },
      "source": [
        "We can see that using Cuda enabled GPU code, the time is very less as comapred to the cpu time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55b_bgrJBhdV"
      },
      "source": [
        "def L2_norm(A):\n",
        "  s=0\n",
        "  for i in A:\n",
        "    s+= i*i\n",
        "  return np.sqrt(s)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44bzIuT2BkFF",
        "outputId": "984014db-582a-4ce2-e639-d8b6e6b04d3c"
      },
      "source": [
        "A = np.random.random(25000000)\n",
        "t1 = time.time()\n",
        "%time L2_norm(A)\n",
        "t2 = time.time()\n",
        "print('Time taken in CPU Norm calculation of vector: {}  s'.format((t2-t1)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 8.02 s, sys: 33.3 ms, total: 8.06 s\n",
            "Wall time: 8.08 s\n",
            "Time taken in CPU Norm calculation of vector: 8.079189777374268  s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54Np3BIkbgRm"
      },
      "source": [
        "@cuda.jit\n",
        "def cuda_norm(a, out):\n",
        "    temp_sum = 0\n",
        "    x = cuda.grid(1)\n",
        "    if(x<a.size):\n",
        "        temp_sum += a[x]**2\n",
        "    out[0] = temp_sum**0.5\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFlcjT5Ohxp-",
        "outputId": "a7b28070-17b3-4315-f209-f9828dc22007"
      },
      "source": [
        "out = np.zeros(1)\n",
        "threadsperblock = 256\n",
        "blockspergrid = math.ceil(A.shape[0]/threadsperblock)\n",
        "t1 = time.time()\n",
        "cuda_norm[blockspergrid, threadsperblock](A, out)\n",
        "t2 = time.time()\n",
        "print(out[0])\n",
        "print(f\"Time to execute in GPU: {t2-t1} s\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7260255853022367\n",
            "Time to execute in GPU: 0.3567492961883545 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4iTtYdzCqbY"
      },
      "source": [
        "A = np.random.rand(500,1000)\n",
        "x = np.ones([1000,1])\n",
        "temp = np.zeros([1000,1])\n",
        "temp1 = np.zeros([1000,1])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7AZut2sCrJF"
      },
      "source": [
        "A_global_mem = cuda.to_device(A)\n",
        "x_global_mem = cuda.to_device(x)\n",
        "temp_global_mem = cuda.to_device(temp)\n",
        "temp1_global_mem = cuda.to_device(temp1)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-EFWW_VCxGv"
      },
      "source": [
        "@cuda.jit\n",
        "def normalize(x, fac, normalized_x):\n",
        "    fac[0][0] = 0\n",
        "    fac[0][0] += x[0][0]\n",
        "    for num in x:\n",
        "        tempmax = abs(num[0])\n",
        "        fac[0][0] = max(fac[0][0], tempmax)\n",
        "    x_max = x[0][0]\n",
        "    for num in x:\n",
        "        x_max = max(x_max, num[0])\n",
        "    for i in range(len(x)):\n",
        "        normalized_x[i][0] = x[i][0]/x_max"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPWKXAdvCte5",
        "outputId": "78761f40-6536-485a-ce2f-e06407e3415a"
      },
      "source": [
        "threadsperblock = 256\n",
        "blockspergrid = math.ceil(A.shape[0] / threadsperblock)\n",
        "\n",
        "t1=time.time()\n",
        "for i in range(8):\n",
        "    fast_matmul[(2,), (4, 4)](A_global_mem, x_global_mem, temp1_global_mem); cuda.synchronize()\n",
        "    lambda_1=np.zeros([1,1])\n",
        "    d_lambda_1=cuda.to_device(lambda_1)\n",
        "    normalize[blockspergrid, threadsperblock](temp1_global_mem,d_lambda_1,temp_global_mem)\n",
        "    print('Eigenvalue {}:{}'.format(i+1, d_lambda_1.copy_to_host()[0][0]))\n",
        "    x_global_mem = temp_global_mem\n",
        "t2=time.time()\n",
        "print('Time taken in cuda enabled GPU Power Method calculation of Eigen vector: {} s'.format(t2-t1))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eigenvalue 1:376.1997198606841\n",
            "Eigenvalue 2:247.82841815426946\n",
            "Eigenvalue 3:247.78349506482482\n",
            "Eigenvalue 4:247.78333277255297\n",
            "Eigenvalue 5:247.78333136439323\n",
            "Eigenvalue 6:247.78333136439323\n",
            "Eigenvalue 7:247.78333136439323\n",
            "Eigenvalue 8:247.78333136439323\n",
            "Time taken in cuda enabled GPU Power Method calculation of Eigen vector: 0.35503721237182617 s\n"
          ]
        }
      ]
    }
  ]
}